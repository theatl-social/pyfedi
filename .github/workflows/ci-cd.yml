name: CI/CD - Build, Test & Validate

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main, develop ]
  workflow_dispatch:
    inputs:
      run_full_tests:
        description: 'Run full production mirror tests'
        required: false
        default: false
        type: boolean

jobs:
  # Fast validation jobs that run on every commit
  startup-validation:
    name: ðŸš€ Startup Validation
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Set up Python 3.13
        uses: actions/setup-python@v5
        with:
          python-version: "3.13"
      
      - name: Cache pip dependencies
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install pytest pytest-mock
      
      - name: Run startup validation tests
        run: |
          echo "ðŸ§ª Running startup validation tests with Python 3.13"
          python -m pytest tests/test_startup_validation.py -v --tb=short
        env:
          SERVER_NAME: test.localhost
          SECRET_KEY: test-secret-key-for-ci
          DATABASE_URL: sqlite:///memory:test.db
          CACHE_TYPE: NullCache
          CACHE_REDIS_URL: memory://
          CELERY_BROKER_URL: memory://localhost/
          TESTING: true
      
      - name: Standalone startup test
        run: |
          echo "ðŸš€ Running standalone startup validation"
          python tests/test_startup_validation.py
        env:
          PYTHONPATH: ${{ github.workspace }}
          SERVER_NAME: test.localhost
          SECRET_KEY: test-secret-key-for-ci
          DATABASE_URL: sqlite:///memory:test.db
          CACHE_TYPE: NullCache
          CACHE_REDIS_URL: memory://
          CELERY_BROKER_URL: memory://localhost/
          TESTING: true

  # Schema immutability and core logic tests
  core-tests:
    name: ðŸ”’ Core Logic Tests
    runs-on: ubuntu-latest
    needs: startup-validation
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Set up Python 3.13
        uses: actions/setup-python@v5
        with:
          python-version: "3.13"
      
      - name: Cache pip dependencies
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install pytest
      
      - name: Run field consistency tests
        run: |
          echo "ðŸ”’ Testing database schema immutability"
          python -m pytest tests/test_field_consistency_simple.py -v
        env:
          SERVER_NAME: test.localhost
          SECRET_KEY: test-secret-key-for-ci
          DATABASE_URL: sqlite:///memory:test.db
          CACHE_TYPE: NullCache
          CACHE_REDIS_URL: memory://
          TESTING: true
      
      - name: Run HTML processing tests
        run: |
          echo "ðŸ§¹ Testing HTML sanitization and processing"
          python -m pytest tests/test_allowlist_html.py -v
        env:
          SERVER_NAME: test.localhost
          SECRET_KEY: test-secret-key-for-ci
          DATABASE_URL: sqlite:///memory:test.db
          CACHE_TYPE: NullCache
          CACHE_REDIS_URL: memory://
          TESTING: true
      
      - name: Run migration consistency tests
        run: |
          echo "ðŸ—„ï¸ Testing database migration consistency"
          python -m pytest tests/test_migration_heads.py -v
        env:
          SERVER_NAME: test.localhost
          SECRET_KEY: test-secret-key-for-ci
          DATABASE_URL: sqlite:///memory:test.db
          CACHE_TYPE: NullCache
          CACHE_REDIS_URL: memory://
          CELERY_BROKER_URL: memory://localhost/
          TESTING: true
      
      - name: Run explore page functionality tests
        run: |
          echo "ðŸ” Testing explore page rendering and content"
          python tests/test_explore_ci_real_world.py
        env:
          SERVER_NAME: test.localhost
          SECRET_KEY: test-secret-key-for-ci
          DATABASE_URL: sqlite:///memory:test.db
          CACHE_TYPE: NullCache
          CACHE_REDIS_URL: memory://
          CELERY_BROKER_URL: memory://localhost/
          TESTING: true
          
      - name: Run additional core tests
        run: |
          echo "ðŸ“ Running additional core logic tests"
          # Collect all test files (excluding ones run separately and tests needing proper fixtures)
          TEST_FILES=$(find tests/ -name "test_*.py" \
            -not -name "test_startup_validation.py" \
            -not -name "test_field_consistency_simple.py" \
            -not -name "test_allowlist_html.py" \
            -not -name "test_migration_heads.py" \
            -not -name "test_explore_ci_real_world.py" \
            -not -name "test_api_*_subscriptions.py" \
            -not -name "test_api_*_bookmarks.py" \
            -not -name "test_api_get_*.py" \
            -not -name "test_api_instance_blocks.py" \
            -not -name "test_activitypub_util.py" \
            -not -name "test_explore_template_render.py" \
            -not -name "test_explore_page_integration.py" \
            -not -name "test_api_blueprint_registration.py" \
            -not -name "test_sql_injection_fixes.py" \
            -not -name "test_user_management_simple.py" \
            -not -name "test_private_registration.py" \
            -not -name "test_private_registration_simple.py" \
            -not -name "test_explore_real_world.py" \
            -not -name "test_private_registration_security.py" \
            -not -name "test_private_registration_security_focused.py")

          # Run all tests in a single pytest command so failures propagate properly
          if [ -n "$TEST_FILES" ]; then
            echo "Running tests: $TEST_FILES"
            python -m pytest $TEST_FILES -v --tb=line
          else
            echo "No additional test files found"
          fi
        env:
          SERVER_NAME: test.localhost
          SECRET_KEY: test-secret-key-for-ci
          DATABASE_URL: sqlite:///memory:test.db
          CACHE_TYPE: NullCache
          CACHE_REDIS_URL: memory://
          TESTING: true

  # Python code quality
  python-quality:
    name: ðŸ Python Code Quality
    runs-on: ubuntu-latest
    # Run in parallel with other jobs, only wait for startup validation
    needs: startup-validation
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Set up Python 3.13
        uses: actions/setup-python@v5
        with:
          python-version: "3.13"
      
      - name: Install linting tools
        run: |
          python -m pip install --upgrade pip
          pip install ruff black isort mypy
          # Install project dependencies for mypy
          pip install -r requirements.txt
      
      - name: Run ruff linting
        run: |
          echo "ðŸ” Running ruff linting"
          ruff check . --output-format=github
        continue-on-error: true
      
      - name: Run black formatting check
        run: |
          echo "ðŸ–¤ Checking code formatting with black"
          black --check --diff .
        continue-on-error: true
      
      - name: Run isort import sorting check
        run: |
          echo "ðŸ“¦ Checking import sorting with isort" 
          isort --check-only --diff .
        continue-on-error: true
      
      - name: Run mypy type checking
        run: |
          echo "ðŸ·ï¸ Running type checking with mypy"
          mypy app/ --ignore-missing-imports --follow-imports=silent
        continue-on-error: true

  # Template linting
  template-quality:
    name: ðŸ“„ Template Quality
    runs-on: ubuntu-latest
    # Run in parallel with other jobs, only wait for startup validation
    needs: startup-validation
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Set up Python 3.13
        uses: actions/setup-python@v5
        with:
          python-version: "3.13"
      
      - name: Cache pip dependencies
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-djlint-${{ hashFiles('.djlintrc') }}
          restore-keys: |
            ${{ runner.os }}-pip-djlint-
      
      - name: Install template validation tools
        run: |
          python -m pip install --upgrade pip
          pip install djlint==1.36.1 jinja2
          echo "ðŸ“¦ djlint version:"
          djlint --version
          echo "ðŸ“¦ jinja2 installed"
      
      - name: Check djlint configuration
        run: |
          if [ -f .djlintrc ]; then
            echo "âœ… Found .djlintrc configuration file"
            cat .djlintrc
          else
            echo "âš ï¸ No .djlintrc file found, using defaults"
          fi
      
      - name: Run template linting
        run: |
          echo "ðŸ“„ Running template linting with djlint"
          echo "Checking for common Jinja2 errors..."
          # First check for any len() usage which is invalid in Jinja2
          if grep -r "len(" app/templates --include="*.html" 2>/dev/null; then
            echo "âŒ ERROR: Found len() usage in templates. Use |length filter instead!"
            exit 1
          fi
          # Run djlint linting (ignore formatting, focus on real errors)
          if [ -f .djlintrc ]; then
            echo "Using .djlintrc configuration"
            djlint app/templates --lint || exit_code=$?
          else
            echo "âš ï¸ No .djlintrc found, using minimal checks"
            djlint app/templates --lint --profile=jinja --ignore="H005,H006,H007,H013,H014,H016,H017,H020,H021,H023,H025,H026,H029,H030,H031,H037,J004,J018,T001,T002,T003,T027,T028,T032" || exit_code=$?
          fi
          if [ "${exit_code:-0}" -ne 0 ]; then
            echo "âŒ Template linting errors found!"
            exit 1
          fi
      
      - name: Template syntax validation
        run: |
          echo "ðŸ” Validating Jinja2 template syntax"
          cat > check_templates.py << 'EOF'
          import os
          import sys
          from jinja2 import Environment, FileSystemLoader, TemplateSyntaxError
          
          # Create environment with custom filters mocked
          env = Environment(loader=FileSystemLoader('app/templates'))
          
          # Add mock custom filters that are defined in the Flask app
          def mock_filter(*args, **kwargs):
              return ''
          
          # Register known custom filters as mocks
          env.filters['shorten'] = mock_filter
          env.filters['shorten_url'] = mock_filter
          env.filters['community_links'] = mock_filter
          env.filters['feed_links'] = mock_filter
          env.filters['person_links'] = mock_filter
          env.filters['remove_images'] = mock_filter
          
          errors = []
          warnings = []
          
          for root, dirs, files in os.walk('app/templates'):
              for file in files:
                  if file.endswith('.html'):
                      template_path = os.path.relpath(os.path.join(root, file), 'app/templates')
                      try:
                          env.get_template(template_path)
                          print(f'âœ… {template_path}')
                      except TemplateSyntaxError as e:
                          # Check if it's a custom filter issue
                          if "No filter named" in str(e):
                              warnings.append(f'âš ï¸ {template_path}: Custom filter - {e}')
                              print(f'âš ï¸ {template_path}: Custom filter - {e}')
                          else:
                              errors.append(f'âŒ {template_path}: {e}')
                              print(f'âŒ {template_path}: {e}')
          
          if errors:
              print(f'\nâŒ Found {len(errors)} template syntax errors!')
              sys.exit(1)
          else:
              print(f'\nâœ… All templates have valid syntax!')
              if warnings:
                  print(f'âš ï¸ {len(warnings)} warnings about custom filters (these are OK)')
          EOF
          python check_templates.py

  # Docker build validation
  docker-validation:
    name: ðŸ³ Docker Build Validation
    runs-on: ubuntu-latest
    needs: startup-validation
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3
      
      - name: Build Docker image
        run: |
          echo "ðŸ³ Building Docker image for validation"
          docker build --target builder -t pyfedi:test .
      
      - name: Test Docker image startup
        run: |
          echo "ðŸ§ª Testing Docker image startup"
          docker run --rm \
            --entrypoint python \
            -e PYTHONPATH=/app \
            -e SERVER_NAME=test.localhost \
            -e SECRET_KEY=test-secret-key \
            -e DATABASE_URL=sqlite:///memory:test.db \
            -e CACHE_TYPE=NullCache \
            -e CACHE_REDIS_URL=memory:// \
            -e CELERY_BROKER_URL=memory://localhost/ \
            -e TESTING=true \
            pyfedi:test \
            tests/test_startup_validation.py

  # Full production mirror tests (optional/conditional)
  production-mirror-tests:
    name: ðŸ­ Production Mirror Tests
    runs-on: ubuntu-latest
    if: ${{ github.event.inputs.run_full_tests == 'true' || github.ref == 'refs/heads/main' || github.event_name == 'pull_request' }}
    needs: [startup-validation, core-tests]
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3
      
      - name: Run production mirror tests
        run: |
          echo "ðŸ­ Running full production mirror test suite"
          chmod +x scripts/run-production-mirror-tests.sh
          ./scripts/run-production-mirror-tests.sh
        timeout-minutes: 15
      
      - name: Upload test logs on failure
        if: failure()
        uses: actions/upload-artifact@v4
        with:
          name: production-mirror-logs
          path: |
            logs/
            *.log
          retention-days: 7

  # Security scanning
  security-scan:
    name: ðŸ” Security Scan
    runs-on: ubuntu-latest
    # Run in parallel with other quality checks
    needs: startup-validation
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Run Bandit security scan
        run: |
          pip install bandit[toml]
          echo "ðŸ” Running security scan with Bandit"
          bandit -r app/ -f json -o bandit-report.json || true
          bandit -r app/ || true
        continue-on-error: true
      
      - name: Upload security report
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: security-report
          path: bandit-report.json
          retention-days: 30

  # Summary job
  ci-summary:
    name: ðŸ“‹ CI Summary
    runs-on: ubuntu-latest
    needs: [startup-validation, core-tests, python-quality, template-quality, docker-validation, security-scan]
    if: always()
    
    steps:
      - name: CI Summary
        run: |
          echo "## ðŸ§ª CI/CD Test Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          if [[ "${{ needs.startup-validation.result }}" == "success" ]]; then
            echo "âœ… **Startup Validation**: Passed" >> $GITHUB_STEP_SUMMARY
          else
            echo "âŒ **Startup Validation**: Failed" >> $GITHUB_STEP_SUMMARY
          fi
          
          if [[ "${{ needs.core-tests.result }}" == "success" ]]; then
            echo "âœ… **Core Tests**: Passed" >> $GITHUB_STEP_SUMMARY
          else
            echo "âŒ **Core Tests**: Failed" >> $GITHUB_STEP_SUMMARY
          fi
          
          if [[ "${{ needs.python-quality.result }}" == "success" ]]; then
            echo "âœ… **Python Code Quality**: Passed" >> $GITHUB_STEP_SUMMARY
          else
            echo "âš ï¸ **Python Code Quality**: Issues found (not blocking)" >> $GITHUB_STEP_SUMMARY
          fi
          
          if [[ "${{ needs.template-quality.result }}" == "success" ]]; then
            echo "âœ… **Template Quality**: Passed" >> $GITHUB_STEP_SUMMARY
          else
            echo "âš ï¸ **Template Quality**: Issues found" >> $GITHUB_STEP_SUMMARY
          fi
          
          if [[ "${{ needs.docker-validation.result }}" == "success" ]]; then
            echo "âœ… **Docker Build**: Passed" >> $GITHUB_STEP_SUMMARY
          else
            echo "âŒ **Docker Build**: Failed" >> $GITHUB_STEP_SUMMARY
          fi
          
          if [[ "${{ needs.security-scan.result }}" == "success" ]]; then
            echo "âœ… **Security Scan**: Passed" >> $GITHUB_STEP_SUMMARY
          else
            echo "âš ï¸ **Security Scan**: Issues found (review recommended)" >> $GITHUB_STEP_SUMMARY
          fi
          
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### ðŸ” Test Coverage" >> $GITHUB_STEP_SUMMARY
          echo "- Flask app initialization and blueprint registration" >> $GITHUB_STEP_SUMMARY
          echo "- Celery worker initialization and task discovery" >> $GITHUB_STEP_SUMMARY
          echo "- Database schema immutability validation" >> $GITHUB_STEP_SUMMARY
          echo "- HTML sanitization and content processing" >> $GITHUB_STEP_SUMMARY
          echo "- Jinja2 template syntax validation and linting" >> $GITHUB_STEP_SUMMARY
          echo "- Docker container startup validation" >> $GITHUB_STEP_SUMMARY
          echo "- Security scanning and code quality checks" >> $GITHUB_STEP_SUMMARY